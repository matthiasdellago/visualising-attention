# Visualising Attention

Have a look at what attention looks like interpreted as a vector field:

- [attention.glsl](https://anvaka.github.io/fieldplay/?dt=0.0009&fo=0.996&dp=0.00109999999999999&cm=3&cx=0.09494999999999987&cy=1.3819999999999997&w=26.997500000000002&h=26.997500000000002&vf=%2F%2F%20Visualising%20the%20field%20that%203%20attention%20vectors%20create%20in%202d%0A%2F%2F%20Play%20with%20the%20parameters%20and%20see%20what%20happens!%0A%2F%2F%20Check%20out%20more%20visualisations%20at%20github.com%2Fmatthiasdellago%2Fvisualising-attention%0A%0Avec2%20attention%28vec2%20p%29%20%7B%0A%20%20%0A%20%20%2F%2F%20init%20v%0A%20%20vec2%20v%20%3D%20vec2%280.%2C%200.%29%3B%0A%20%20%0A%20%20%2F%2F%20scale%20defined%20separately%20to%20easily%20change%20it.%20Try%20it!%0A%20%20float%20scale%20%3D%203.%3B%0A%20%20%0A%20%20%2F%2Fdefine%20attention%20vectors%0A%20%20vec2%20f1%20%3D%20scale%20*%20vec2%280.%2C1.5%29%3B%0A%20%20vec2%20f2%20%3D%20scale%20*%20vec2%281.%2C0.%29%3B%0A%20%20vec2%20f3%20%3D%20scale%20*%20vec2%28-1.5%2C0%29%3B%0A%0A%20%20%2F%2F%20calc%20projections%20-%3E%20weights%0A%20%20float%20w1%20%3D%20dot%28p%2Cf1%29%3B%0A%20%20float%20w2%20%3D%20dot%28p%2Cf2%29%3B%0A%20%20float%20w3%20%3D%20dot%28p%2Cf3%29%3B%0A%0A%20%20%2F%2F%20softmax%0A%20%20float%20sum%20%3D%20exp%28w1%29%2Bexp%28w2%29%2Bexp%28w3%29%3B%0A%20%20w1%20%3D%20exp%28w1%29%2Fsum%3B%0A%20%20w2%20%3D%20exp%28w2%29%2Fsum%3B%0A%20%20w3%20%3D%20exp%28w3%29%2Fsum%3B%0A%0A%20%20%2F%2F%20linear%20combination%0A%20%20vec2%20new_p%20%3D%20w1*f1%20%2B%20w2*f2%20%2B%20w3*f3%3B%0A%20%20%0A%20%20%2F%2F%20calc%20delta%20of%20p%0A%20%20v%20%3D%20new_p%20-%20p%3B%0A%0A%20%20return%20v%3B%0A%7D%0A%0Avec2%20get_velocity%28vec2%20p%29%7B%0A%20%20%2F%2F%20get%20p.x%20and%20p.y%2C%20the%20current%20coordinates%0A%20%20%2F%2F%20return%20v.x%20and%20v.y%2C%20the%20velocity%20at%20point%20p%0A%20%20return%20attention%28p%29%3B%0A%7D&code=%2F%2F%20Visualising%20the%20field%20that%203%20attention%20vectors%20create%20in%202d%0A%2F%2F%20Play%20with%20the%20parameters%20and%20see%20what%20happens!%0A%2F%2F%20Check%20out%20more%20visualisations%20at%20github.com%2Fmatthiasdellago%2Fvisualising-attention%0A%0Avec2%20attention%28vec2%20p%29%20%7B%0A%20%20%0A%20%20%2F%2F%20init%20v%0A%20%20vec2%20v%20%3D%20vec2%280.%2C%200.%29%3B%0A%20%20%0A%20%20%2F%2F%20scale%20defined%20separately%20to%20easily%20change%20it.%20Try%20it!%0A%20%20float%20scale%20%3D%203.%3B%0A%20%20%0A%20%20%2F%2Fdefine%20attention%20vectors%0A%20%20vec2%20f1%20%3D%20scale%20*%20vec2%280.%2C1.5%29%3B%0A%20%20vec2%20f2%20%3D%20scale%20*%20vec2%281.%2C0.%29%3B%0A%20%20vec2%20f3%20%3D%20scale%20*%20vec2%28-1.5%2C0%29%3B%0A%0A%20%20%2F%2F%20calc%20projections%20-%3E%20weights%0A%20%20float%20w1%20%3D%20dot%28p%2Cf1%29%3B%0A%20%20float%20w2%20%3D%20dot%28p%2Cf2%29%3B%0A%20%20float%20w3%20%3D%20dot%28p%2Cf3%29%3B%0A%0A%20%20%2F%2F%20softmax%0A%20%20float%20sum%20%3D%20exp%28w1%29%2Bexp%28w2%29%2Bexp%28w3%29%3B%0A%20%20w1%20%3D%20exp%28w1%29%2Fsum%3B%0A%20%20w2%20%3D%20exp%28w2%29%2Fsum%3B%0A%20%20w3%20%3D%20exp%28w3%29%2Fsum%3B%0A%0A%20%20%2F%2F%20linear%20combination%0A%20%20vec2%20new_p%20%3D%20w1*f1%20%2B%20w2*f2%20%2B%20w3*f3%3B%0A%20%20%0A%20%20%2F%2F%20calc%20delta%20of%20p%0A%20%20v%20%3D%20new_p%20-%20p%3B%0A%0A%20%20return%20v%3B%0A%7D%0A%0Avec2%20get_velocity%28vec2%20p%29%7B%0A%20%20%2F%2F%20get%20p.x%20and%20p.y%2C%20the%20current%20coordinates%0A%20%20%2F%2F%20return%20v.x%20and%20v.y%2C%20the%20velocity%20at%20point%20p%0A%20%20return%20attention%28p%29%3B%0A%7D&pc=90000)

Play with the QK-matrix!

- [attention_QK-matrix.glsl](https://anvaka.github.io/fieldplay/?dt=0.0009&fo=0.996&dp=0.00109999999999999&cm=3&cx=0.09865000000000013&cy=1.3820000000000006&w=27.0049&h=27.0049&vf=%2F%2F%20Visualising%20the%20field%20that%203%20attention%20vectors%20create%20in%202d%0A%2F%2F%20Play%20with%20the%20parameters%20and%20see%20what%20happens!%0A%2F%2F%20Now%20with%20built%20in%20quasi-kernel%20and%20your%20very%20own%20QK-matrix!%0A%2F%2F%20Check%20out%20more%20visualisations%20at%20github.com%2Fmatthiasdellago%2Fvisualising-attention%0A%0Avec2%20attention%28vec2%20p%29%20%7B%0A%20%20%0A%20%20%2F%2F%20init%20v%0A%20%20vec2%20v%20%3D%20vec2%280.%2C%200.%29%3B%0A%20%20%0A%20%20%2F%2F%20scale%20defined%20separately%20to%20easily%20change%20it.%20Try%20it!%0A%20%20float%20scale%20%3D%203.%3B%0A%20%20%0A%20%20%2F%2Fdefine%20attention%20vectors%0A%20%20vec2%20f1%20%3D%20scale%20*%20vec2%280.%2C1.5%29%3B%0A%20%20vec2%20f2%20%3D%20scale%20*%20vec2%281.%2C0.%29%3B%0A%20%20vec2%20f3%20%3D%20scale%20*%20vec2%28-1.5%2C0%29%3B%0A%0A%20%20%0A%20%20%2F%2FQK-matrix%0A%20%20%2F%2Fchange%20this%2C%20its%20awesome!%0A%20%20%2F%2Fmat2%20QK%3Dmat2%280.%2C1.%2C1.%2C0.%29%3B%0A%20%20mat2%20QK%3Dmat2%28-1.%2C0.%2C0.%2C-1.%29%3B%0A%20%20%0A%20%20%2F%2F%20calc%20projections%20-%3E%20weights%0A%20%20float%20w1%20%3D%20dot%28p%2CQK*f1%29%3B%0A%20%20float%20w2%20%3D%20dot%28p%2CQK*f2%29%3B%0A%20%20float%20w3%20%3D%20dot%28p%2CQK*f3%29%3B%0A%0A%20%20%2F%2F%20softmax%0A%20%20float%20sum%20%3D%20exp%28w1%29%2Bexp%28w2%29%2Bexp%28w3%29%3B%0A%20%20w1%20%3D%20exp%28w1%29%2Fsum%3B%0A%20%20w2%20%3D%20exp%28w2%29%2Fsum%3B%0A%20%20w3%20%3D%20exp%28w3%29%2Fsum%3B%0A%0A%20%20%2F%2F%20linear%20combination%0A%20%20vec2%20new_p%20%3D%20w1*f1%20%2B%20w2*f2%20%2B%20w3*f3%3B%0A%20%20%0A%20%20%2F%2F%20calc%20delta%20of%20p%0A%20%20v%20%3D%20new_p%20-%20p%3B%0A%0A%20%20return%20v%3B%0A%7D%0A%0Avec2%20get_velocity%28vec2%20p%29%7B%0A%20%20%2F%2F%20get%20p.x%20and%20p.y%2C%20the%20current%20coordinates%0A%20%20%2F%2F%20return%20v.x%20and%20v.y%2C%20the%20velocity%20at%20point%20p%0A%20%20return%20attention%28p%29%3B%0A%7D&code=%2F%2F%20Visualising%20the%20field%20that%203%20attention%20vectors%20create%20in%202d%0A%2F%2F%20Play%20with%20the%20parameters%20and%20see%20what%20happens!%0A%2F%2F%20Check%20out%20more%20visualisations%20at%20github.com%2Fmatthiasdellago%2Fvisualising-attention%0A%0Avec2%20attention%28vec2%20p%29%20%7B%0A%20%20%0A%20%20%2F%2F%20init%20v%0A%20%20vec2%20v%20%3D%20vec2%280.%2C%200.%29%3B%0A%20%20%0A%20%20%2F%2F%20scale%20defined%20separately%20to%20easily%20change%20it.%20Try%20it!%0A%20%20float%20scale%20%3D%203.%3B%0A%20%20%0A%20%20%2F%2Fdefine%20attention%20vectors%0A%20%20vec2%20f1%20%3D%20scale%20*%20vec2%280.%2C1.5%29%3B%0A%20%20vec2%20f2%20%3D%20scale%20*%20vec2%281.%2C0.%29%3B%0A%20%20vec2%20f3%20%3D%20scale%20*%20vec2%28-1.5%2C0%29%3B%0A%0A%20%20%2F%2F%20calc%20projections%20-%3E%20weights%0A%20%20float%20w1%20%3D%20dot%28p%2Cf1%29%3B%0A%20%20float%20w2%20%3D%20dot%28p%2Cf2%29%3B%0A%20%20float%20w3%20%3D%20dot%28p%2Cf3%29%3B%0A%0A%20%20%2F%2F%20softmax%0A%20%20float%20sum%20%3D%20exp%28w1%29%2Bexp%28w2%29%2Bexp%28w3%29%3B%0A%20%20w1%20%3D%20exp%28w1%29%2Fsum%3B%0A%20%20w2%20%3D%20exp%28w2%29%2Fsum%3B%0A%20%20w3%20%3D%20exp%28w3%29%2Fsum%3B%0A%0A%20%20%2F%2F%20linear%20combination%0A%20%20vec2%20new_p%20%3D%20w1*f1%20%2B%20w2*f2%20%2B%20w3*f3%3B%0A%20%20%0A%20%20%2F%2F%20calc%20delta%20of%20p%0A%20%20v%20%3D%20new_p%20-%20p%3B%0A%0A%20%20return%20v%3B%0A%7D%0A%0Avec2%20get_velocity%28vec2%20p%29%7B%0A%20%20%2F%2F%20get%20p.x%20and%20p.y%2C%20the%20current%20coordinates%0A%20%20%2F%2F%20return%20v.x%20and%20v.y%2C%20the%20velocity%20at%20point%20p%0A%20%20return%20attention%28p%29%3B%0A%7D&pc=90000)

Here is an example of a strong attractor:

- [strong_attractor.glsl](https://anvaka.github.io/fieldplay/?dt=0.0009&fo=0.996&dp=0.00109999999999999&cm=3&cx=-1.5661500000000004&cy=0.17804999999999982&w=25.1461&h=25.1461&vf=%2F%2F%20Example%20of%20a%20strong%20attractor.%0A%2F%2F%20One%20attention%20vector%20%20is%20stationary%20at%20%286%2C0%29.%0A%2F%2F%20Another%20is%20orbiting%20the%20origin%20with%20a%20radius%20of%203.%0A%2F%2F%20When%20close%20to%20the%20larger%20attention%20vector%20the%20smaller%20vectors%20attractive%20field%20is%20eclipsed%2C%0A%2F%2F%20and%20it%20no%20longer%20forms%20it%27s%20own%20attractor.%0A%2F%2F%20The%20particle%20colour%20depends%20on%20the%20direction%20of%20its%20velocity%2C%0A%2F%2F%20so%20an%20attrator%20%28i.e.%20a%20point%20where%20particles%20converge%29%20is%20easily%20identifiable%20because%20all%20colours%20meet%20there.%0A%2F%2F%20Play%20with%20the%20parameters%20and%20see%20what%20happens!%0A%2F%2F%20Check%20out%20more%20visualisations%20at%20github.com%2Fmatthiasdellago%2Fvisualising-attention%0A%0Avec2%20attention%28vec2%20p%29%20%7B%0A%20%20%0A%20%20%2F%2F%20init%20v%0A%20%20vec2%20v%20%3D%20vec2%280.%2C%200.%29%3B%0A%20%20%0A%20%20%2F%2F%20scale%20defined%20separately%20to%20easily%20change%20it.%20Try%20it!%0A%20%20float%20scale%20%3D%203.%3B%0A%20%20%0A%20%20%2F%2F%20using%20frame%20counter%20to%20create%20oscillating%20movement.%0A%20%20float%20freq%20%3D%200.0005%3B%0A%20%20float%20amplitude%20%3D%202.%3B%0A%20%20float%20osc%20%3D%20amplitude*fract%28frame*freq%29%3B%0A%20%20%0A%20%20%2F%2Fdefine%20attention%20vectors%0A%20%20vec2%20f1%20%3D%20scale%20*%20vec2%282.%2C0.%29%3B%0A%20%20vec2%20f2%20%3D%20scale%20*%20vec2%28cos%282.*3.14*osc%29%2Csin%282.*3.14*osc%29%29%3B%0A%0A%20%20%2F%2F%20calc%20projections%20-%3E%20weights%0A%20%20float%20w1%20%3D%20dot%28p%2Cf1%29%3B%0A%20%20float%20w2%20%3D%20dot%28p%2Cf2%29%3B%0A%0A%20%20%2F%2F%20softmax%0A%20%20float%20sum%20%3D%20exp%28w1%29%2Bexp%28w2%29%3B%0A%20%20w1%20%3D%20exp%28w1%29%2Fsum%3B%0A%20%20w2%20%3D%20exp%28w2%29%2Fsum%3B%0A%0A%20%20%2F%2F%20linear%20combination%0A%20%20vec2%20new_p%20%3D%20w1*f1%20%2B%20w2*f2%3B%0A%20%20%0A%20%20%2F%2F%20calc%20delta%20of%20p%0A%20%20v%20%3D%20new_p%20-%20p%3B%0A%0A%20%20return%20v%3B%0A%7D%0A%0Avec2%20get_velocity%28vec2%20p%29%7B%0A%20%20%2F%2F%20get%20p.x%20and%20p.y%2C%20the%20current%20coordinates%0A%20%20%2F%2F%20return%20v.x%20and%20v.y%2C%20the%20velocity%20at%20point%20p%0A%20%20return%20attention%28p%29%3B%0A%7D&code=%2F%2F%20Example%20of%20a%20strong%20attractor.%0A%2F%2F%20One%20attention%20vector%20%20is%20stationary%20at%20%286%2C0%29.%0A%2F%2F%20Another%20is%20orbiting%20the%20origin%20with%20a%20radius%20of%203.%0A%2F%2F%20When%20close%20to%20the%20larger%20attention%20vector%20the%20smaller%20vectors%20attractive%20field%20is%20eclipsed%2C%0A%2F%2F%20and%20it%20no%20longer%20forms%20it%27s%20own%20attractor.%0A%2F%2F%20The%20particle%20colour%20depends%20on%20the%20direction%20of%20its%20velocity%2C%0A%2F%2F%20so%20an%20attrator%20%28i.e.%20a%20point%20where%20particles%20converge%29%20is%20easily%20identifiable%20because%20all%20colours%20meet%20there.%0A%2F%2F%20Play%20with%20the%20parameters%20and%20see%20what%20happens!%0A%2F%2F%20Check%20out%20more%20visualisations%20at%20github.com%2Fmatthiasdellago%2Fvisualising-attention%0A%0Avec2%20attention%28vec2%20p%29%20%7B%0A%20%20%0A%20%20%2F%2F%20init%20v%0A%20%20vec2%20v%20%3D%20vec2%280.%2C%200.%29%3B%0A%20%20%0A%20%20%2F%2F%20scale%20defined%20separately%20to%20easily%20change%20it.%20Try%20it!%0A%20%20float%20scale%20%3D%203.%3B%0A%20%20%0A%20%20%2F%2F%20using%20frame%20counter%20to%20create%20oscillating%20movement.%0A%20%20float%20freq%20%3D%200.0005%3B%0A%20%20float%20amplitude%20%3D%202.%3B%0A%20%20float%20osc%20%3D%20amplitude*fract%28frame*freq%29%3B%0A%20%20%0A%20%20%2F%2Fdefine%20attention%20vectors%0A%20%20vec2%20f1%20%3D%20scale%20*%20vec2%282.%2C0.%29%3B%0A%20%20vec2%20f2%20%3D%20scale%20*%20vec2%28cos%282.*3.14*osc%29%2Csin%282.*3.14*osc%29%29%3B%0A%0A%20%20%2F%2F%20calc%20projections%20-%3E%20weights%0A%20%20float%20w1%20%3D%20dot%28p%2Cf1%29%3B%0A%20%20float%20w2%20%3D%20dot%28p%2Cf2%29%3B%0A%0A%20%20%2F%2F%20softmax%0A%20%20float%20sum%20%3D%20exp%28w1%29%2Bexp%28w2%29%3B%0A%20%20w1%20%3D%20exp%28w1%29%2Fsum%3B%0A%20%20w2%20%3D%20exp%28w2%29%2Fsum%3B%0A%0A%20%20%2F%2F%20linear%20combination%0A%20%20vec2%20new_p%20%3D%20w1*f1%20%2B%20w2*f2%3B%0A%20%20%0A%20%20%2F%2F%20calc%20delta%20of%20p%0A%20%20v%20%3D%20new_p%20-%20p%3B%0A%0A%20%20return%20v%3B%0A%7D%0A%0Avec2%20get_velocity%28vec2%20p%29%7B%0A%20%20%2F%2F%20get%20p.x%20and%20p.y%2C%20the%20current%20coordinates%0A%20%20%2F%2F%20return%20v.x%20and%20v.y%2C%20the%20velocity%20at%20point%20p%0A%20%20return%20attention%28p%29%3B%0A%7D&pc=90000)

How does changing the magnitude of the pre-softmax weights affect the field?

- [attention_varying_scale.glsl](https://anvaka.github.io/fieldplay/?dt=0.0009&fo=0.996&dp=0.00109999999999999&cm=3&cx=-0.5656999999999999&cy=0.11795&w=7.182&h=7.182&vf=%2F%2F%20Visualising%20the%20field%20that%203%20attention%20vectors%20create%20in%202d%0A%2F%2F%20Here%20we%20illustrate%20how%20the%20nature%20of%20attention%20changes%20as%20the%20scale%20changes.%0A%2F%2F%20We%20keep%20all%20attention%20vectors%2Ftokens%20stationary%2C%0A%2F%2F%20but%20vary%20the%20magnitude%20of%20all%20weights%20before%20the%20softmax%20operation%20by%20the%20same%20factor.%0A%2F%2F%20Due%20to%20the%20exponential%20nature%20of%20softmax%20if%20we%20scale%20all%20up%20all%20wieghts%2C%0A%2F%2F%20post%20softmax%20one%20weight%20becomes%20much%20more%20dominant%2C%20which%20we%20can%20see%20as%20more%20clearly%20defined%20attractors.%0A%2F%2F%20This%20more%20closely%20approximates%20a%20max%28%29%20operation%3A%20https%3A%2F%2Fgithub.com%2Fmatthiasdellago%2Fvisualising-attention%2Fblob%2Fmain%2Fattention_hardmax.glsl%0A%0A%2F%2F%20Play%20with%20the%20parameters%20and%20see%20what%20happens!%0A%2F%2F%20Check%20out%20more%20visualisations%20at%20github.com%2Fmatthiasdellago%2Fvisualising-attention%0A%0Avec2%20attention%28vec2%20p%29%20%7B%0A%20%20%0A%20%20%2F%2F%20init%20v%0A%20%20vec2%20v%20%3D%20vec2%280.%2C%200.%29%3B%0A%20%20%0A%20%20float%20freq%20%3D%200.001%3B%0A%20%20float%20exponent%20%3D%202.%3B%0A%20%20float%20osc%20%3D%20pow%282.*fract%28frame*freq%29-1.%2Cexponent%29%3B%0A%20%20%2F%2F%20scale%20defined%20separately%20to%20easily%20change%20it.%20Try%20it!%0A%20%20float%20scale%20%3D%201.*min%2810.%2C1.%2F%28osc%2B0.000001%29%29%3B%0A%20%20%0A%20%20%0A%20%20%2F%2Fdefine%20attention%20vectors%0A%20%20vec2%20f1%20%3D%20vec2%280.%2C1.5%29%3B%0A%20%20vec2%20f2%20%3D%20vec2%281.%2C0.%29%3B%0A%20%20vec2%20f3%20%3D%20vec2%28-1.5%2C0%29%3B%0A%0A%20%20%2F%2F%20calc%20projections%20-%3E%20weights%0A%20%20float%20w1%20%3D%20dot%28p%2Cf1%29*scale%3B%0A%20%20float%20w2%20%3D%20dot%28p%2Cf2%29*scale%3B%0A%20%20float%20w3%20%3D%20dot%28p%2Cf3%29*scale%3B%0A%0A%20%20%2F%2F%20softmax%0A%20%20float%20sum%20%3D%20exp%28w1%29%2Bexp%28w2%29%2Bexp%28w3%29%3B%0A%20%20w1%20%3D%20exp%28w1%29%2Fsum%3B%0A%20%20w2%20%3D%20exp%28w2%29%2Fsum%3B%0A%20%20w3%20%3D%20exp%28w3%29%2Fsum%3B%0A%0A%20%20%2F%2F%20linear%20combination%0A%20%20vec2%20new_p%20%3D%20w1*f1%20%2B%20w2*f2%20%2B%20w3*f3%3B%0A%20%20%0A%20%20%2F%2F%20calc%20delta%20of%20p%0A%20%20v%20%3D%20new_p%20-%20p%3B%0A%0A%20%20return%20v%3B%0A%7D%0A%0Avec2%20get_velocity%28vec2%20p%29%7B%0A%20%20%2F%2F%20get%20p.x%20and%20p.y%2C%20the%20current%20coordinates%0A%20%20%2F%2F%20return%20v.x%20and%20v.y%2C%20the%20velocity%20at%20point%20p%0A%20%20return%20attention%28p%29%3B%0A%7D&code=%2F%2F%20Visualising%20the%20field%20that%203%20attention%20vectors%20create%20in%202d%0A%2F%2F%20Here%20we%20illustrate%20how%20the%20nature%20of%20attention%20changes%20as%20the%20scale%20changes.%0A%2F%2F%20We%20keep%20all%20attention%20vectors%2Ftokens%20stationary%2C%0A%2F%2F%20but%20vary%20the%20magnitude%20of%20all%20weights%20before%20the%20softmax%20operation%20by%20the%20same%20factor.%0A%2F%2F%20Due%20to%20the%20exponential%20nature%20of%20softmax%20if%20we%20scale%20all%20up%20all%20wieghts%2C%0A%2F%2F%20post%20softmax%20one%20weight%20becomes%20much%20more%20dominant%2C%20which%20we%20can%20see%20as%20more%20clearly%20defined%20attractors.%0A%2F%2F%20This%20more%20closely%20approximates%20a%20max%28%29%20operation%3A%20https%3A%2F%2Fgithub.com%2Fmatthiasdellago%2Fvisualising-attention%2Fblob%2Fmain%2Fattention_hardmax.glsl%0A%0A%2F%2F%20Play%20with%20the%20parameters%20and%20see%20what%20happens!%0A%2F%2F%20Check%20out%20more%20visualisations%20at%20github.com%2Fmatthiasdellago%2Fvisualising-attention%0A%0Avec2%20attention%28vec2%20p%29%20%7B%0A%20%20%0A%20%20%2F%2F%20init%20v%0A%20%20vec2%20v%20%3D%20vec2%280.%2C%200.%29%3B%0A%20%20%0A%20%20float%20freq%20%3D%200.001%3B%0A%20%20float%20exponent%20%3D%202.%3B%0A%20%20float%20osc%20%3D%20pow%282.*fract%28frame*freq%29-1.%2Cexponent%29%3B%0A%20%20%2F%2F%20scale%20defined%20separately%20to%20easily%20change%20it.%20Try%20it!%0A%20%20float%20scale%20%3D%201.*min%2810.%2C1.%2F%28osc%2B0.000001%29%29%3B%0A%20%20%0A%20%20%0A%20%20%2F%2Fdefine%20attention%20vectors%0A%20%20vec2%20f1%20%3D%20vec2%280.%2C1.5%29%3B%0A%20%20vec2%20f2%20%3D%20vec2%281.%2C0.%29%3B%0A%20%20vec2%20f3%20%3D%20vec2%28-1.5%2C0%29%3B%0A%0A%20%20%2F%2F%20calc%20projections%20-%3E%20weights%0A%20%20float%20w1%20%3D%20dot%28p%2Cf1%29*scale%3B%0A%20%20float%20w2%20%3D%20dot%28p%2Cf2%29*scale%3B%0A%20%20float%20w3%20%3D%20dot%28p%2Cf3%29*scale%3B%0A%0A%20%20%2F%2F%20softmax%0A%20%20float%20sum%20%3D%20exp%28w1%29%2Bexp%28w2%29%2Bexp%28w3%29%3B%0A%20%20w1%20%3D%20exp%28w1%29%2Fsum%3B%0A%20%20w2%20%3D%20exp%28w2%29%2Fsum%3B%0A%20%20w3%20%3D%20exp%28w3%29%2Fsum%3B%0A%0A%20%20%2F%2F%20linear%20combination%0A%20%20vec2%20new_p%20%3D%20w1*f1%20%2B%20w2*f2%20%2B%20w3*f3%3B%0A%20%20%0A%20%20%2F%2F%20calc%20delta%20of%20p%0A%20%20v%20%3D%20new_p%20-%20p%3B%0A%0A%20%20return%20v%3B%0A%7D%0A%0Avec2%20get_velocity%28vec2%20p%29%7B%0A%20%20%2F%2F%20get%20p.x%20and%20p.y%2C%20the%20current%20coordinates%0A%20%20%2F%2F%20return%20v.x%20and%20v.y%2C%20the%20velocity%20at%20point%20p%0A%20%20return%20attention%28p%29%3B%0A%7D&pc=90000)

You can also copy paste one of the glsl files into [fieldplay](https://anvaka.github.io/fieldplay) or direcly include one of my scrips by wiriting a line like this into the code field:

```
#include https://raw.githubusercontent.com/matthiasdellago/visualising-attention/main/attention.glsl
```
